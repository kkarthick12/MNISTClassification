{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers - Group Members\n",
    "\n",
    "- Abhijit\n",
    "- Biyun\n",
    "- Hayoung Kim\n",
    "- Karthick Vel Kathirvel\n",
    "- Rahull Borana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "m = 7\n",
    "block_size = 16\n",
    "hidden_dim = 64\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "key_dim = 8\n",
    "mlp_dim = 128\n",
    "dropout_rate = 0.05\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is written as a tensorflow \"layer\".  it's just a vector the same size as the \n",
    "# output of the previous layer. the vector is initialized randomly, but we'll use \n",
    "# gradient descent to update the values in the vector\n",
    "#\n",
    "# it's purpose is to be appended to the beginning of the sequence of vectors fed into\n",
    "# the transformer.  then after the transformer runs on the whole data, we just grab\n",
    "# the resulting zero-th vector...the class token...and use that as the portfolio weights\n",
    "class ClassToken(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
    "            trainable = True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        hidden_dim = self.w.shape[-1]\n",
    "\n",
    "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
    "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ViT(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes):\n",
    "    # n is number of rows of blocks\n",
    "    # m is number of cols of blocks\n",
    "    # block_size is number of pixels (with rgb) in each block\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=(n*m,block_size))\n",
    "    inp2 = tf.keras.layers.Input(shape=(n*m))\n",
    "    mid = tf.keras.layers.Dense(hidden_dim)(inp) # transform to vectors with different dimension\n",
    "    # the positional embeddings\n",
    "#     positions = tf.range(start=0, limit=n*m, delta=1)\n",
    "    emb = tf.keras.layers.Embedding(input_dim=n*m, output_dim=hidden_dim)(inp2) # learned positional embedding for each of the n*m possible possitions\n",
    "    mid = mid + emb # for some reason, tf.keras.layers.Add causes an error, but + doesn't?\n",
    "    # create and append class token to beginning of all input vectors\n",
    "    token = ClassToken()(mid) # append class token to beginning of sequence\n",
    "    mid = tf.keras.layers.Concatenate(axis=1)([token, mid])\n",
    "    \n",
    "    for l in range(num_layers): # how many Transformer Head layers are there?\n",
    "        ln  = tf.keras.layers.LayerNormalization()(mid) # normalize\n",
    "        mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=key_dim,value_dim=key_dim)(ln,ln,ln) # self attention!\n",
    "        add = tf.keras.layers.Add()([mid,mha]) # add and norm\n",
    "        ln  = tf.keras.layers.LayerNormalization()(add)\n",
    "        den = tf.keras.layers.Dense(mlp_dim,activation='gelu')(ln) # maybe should be relu...who knows...\n",
    "        den = tf.keras.layers.Dropout(dropout_rate)(den) # regularization\n",
    "        den = tf.keras.layers.Dense(hidden_dim)(den) # back to the right dimensional space\n",
    "        den = tf.keras.layers.Dropout(dropout_rate)(den)\n",
    "        mid = tf.keras.layers.Add()([den,add]) # add and norm again\n",
    "    ln = tf.keras.layers.LayerNormalization()(mid)\n",
    "    fl = ln[:,0,:] # just grab the class token for each image in batch\n",
    "    clas = tf.keras.layers.Dense(num_classes,activation='softmax')(fl) # probability that the image is in each category\n",
    "    mod = tf.keras.models.Model([inp,inp2],clas)\n",
    "    mod.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 49, 16)]     0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 49)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 49, 64)       1088        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 49, 64)       3136        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 49, 64)      0           ['dense[0][0]',                  \n",
      " da)                                                              'embedding[0][0]']              \n",
      "                                                                                                  \n",
      " class_token (ClassToken)       (None, 1, 64)        64          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 50, 64)       0           ['class_token[0][0]',            \n",
      "                                                                  'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 50, 64)      128         ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 50, 64)      16640       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]',    \n",
      "                                                                  'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 50, 64)       0           ['concatenate[0][0]',            \n",
      "                                                                  'multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 50, 64)      128         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 50, 128)      8320        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 50, 128)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50, 64)       8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50, 64)       0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 50, 64)       0           ['dropout_1[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 50, 64)      128         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 50, 64)      16640       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]',  \n",
      "                                                                  'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 50, 64)       0           ['add_1[0][0]',                  \n",
      "                                                                  'multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 50, 64)      128         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 50, 128)      8320        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 50, 128)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 50, 64)       8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 50, 64)       0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 50, 64)       0           ['dropout_3[0][0]',              \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 50, 64)      128         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 50, 64)      16640       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]',  \n",
      "                                                                  'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 50, 64)       0           ['add_3[0][0]',                  \n",
      "                                                                  'multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 50, 64)      128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 50, 128)      8320        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 50, 128)      0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 50, 64)       8256        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 50, 64)       0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 50, 64)       0           ['dropout_5[0][0]',              \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 50, 64)      128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 50, 64)      16640       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]',  \n",
      "                                                                  'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 50, 64)       0           ['add_5[0][0]',                  \n",
      "                                                                  'multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 50, 64)      128         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 50, 128)      8320        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 50, 128)      0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 50, 64)       8256        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 50, 64)       0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 50, 64)       0           ['dropout_7[0][0]',              \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 50, 64)      128         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 50, 64)      16640       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]',  \n",
      "                                                                  'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 50, 64)       0           ['add_7[0][0]',                  \n",
      "                                                                  'multi_head_attention_4[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 50, 64)      128         ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 50, 128)      8320        ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 50, 128)      0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 50, 64)       8256        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 50, 64)       0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 50, 64)       0           ['dropout_9[0][0]',              \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 50, 64)      128         ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 50, 64)      16640       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]', \n",
      "                                                                  'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 50, 64)       0           ['add_9[0][0]',                  \n",
      "                                                                  'multi_head_attention_5[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 50, 64)      128         ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 50, 128)      8320        ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 50, 128)      0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 50, 64)       8256        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 50, 64)       0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 50, 64)       0           ['dropout_11[0][0]',             \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 50, 64)      128         ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 64)          0           ['layer_normalization_12[0][0]'] \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 10)           650         ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 205,898\n",
      "Trainable params: 205,898\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trans = build_ViT(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes)\n",
    "trans.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "ndata_train = x_train.shape[0]\n",
    "ndata_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ravel = np.zeros((ndata_train,n*m,block_size))\n",
    "for img in range(ndata_train):\n",
    "    ind = 0\n",
    "    for row in range(n):\n",
    "        for col in range(m):\n",
    "            x_train_ravel[img,ind,:] = x_train[img,(row*4):((row+1)*4),(col*4):((col+1)*4)].ravel()\n",
    "            ind += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_ravel = np.zeros((ndata_test,n*m,block_size))\n",
    "for img in range(ndata_test):\n",
    "    ind = 0\n",
    "    for row in range(n):\n",
    "        for col in range(m):\n",
    "            x_test_ravel[img,ind,:] = x_test[img,(row*4):((row+1)*4),(col*4):((col+1)*4)].ravel()\n",
    "            ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feed_train = np.array([list(range(n*m))]*ndata_train)\n",
    "pos_feed_test = np.array([list(range(n*m))]*ndata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.fit([x_train_ravel,pos_feed_train],y_train,epochs=40,batch_size = 40,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ClassToken(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):  \n",
    "        super(ClassToken, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(1, 1, input_shape[-1]), \n",
    "            initializer=w_init,\n",
    "            trainable=True,\n",
    "            name='cls_token'  \n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        hidden_dim = self.w.shape[-1]\n",
    "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
    "        return tf.cast(cls, dtype=inputs.dtype)\n",
    "\n",
    "model_path = 'ViT_validationAcc9867_updated.h5'  # Path to your model file\n",
    "#model_path = 'ViT_validationAcc9867.h5'  # Path to your model file\n",
    "loaded_model = tf.keras.models.load_model(model_path, custom_objects={'ClassToken': ClassToken})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "m = 7\n",
    "block_size = 16\n",
    "hidden_dim = 64\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "key_dim = 8\n",
    "mlp_dim = 128\n",
    "dropout_rate = 0.05\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "ndata_train = x_train.shape[0]\n",
    "ndata_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ravel = np.zeros((ndata_train,n*m,block_size))\n",
    "for img in range(ndata_train):\n",
    "    ind = 0\n",
    "    for row in range(n):\n",
    "        for col in range(m):\n",
    "            x_train_ravel[img,ind,:] = x_train[img,(row*4):((row+1)*4),(col*4):((col+1)*4)].ravel()\n",
    "            ind += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_ravel = np.zeros((ndata_test,n*m,block_size))\n",
    "for img in range(ndata_test):\n",
    "    ind = 0\n",
    "    for row in range(n):\n",
    "        for col in range(m):\n",
    "            x_test_ravel[img,ind,:] = x_test[img,(row*4):((row+1)*4),(col*4):((col+1)*4)].ravel()\n",
    "            ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feed_train = np.array([list(range(n*m))]*ndata_train)\n",
    "pos_feed_test = np.array([list(range(n*m))]*ndata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1500/1500 [==============================] - 21s 10ms/step - loss: 0.0273 - accuracy: 0.9915\n",
      "Epoch 2/40\n",
      "1500/1500 [==============================] - 15s 10ms/step - loss: 0.0204 - accuracy: 0.9931\n",
      "Epoch 3/40\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.0198 - accuracy: 0.9929\n",
      "Epoch 4/40\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.0185 - accuracy: 0.9938\n",
      "Epoch 5/40\n",
      "1500/1500 [==============================] - 15s 10ms/step - loss: 0.0174 - accuracy: 0.9944\n",
      "Epoch 6/40\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.0181 - accuracy: 0.9938\n",
      "Epoch 7/40\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.0156 - accuracy: 0.9947\n",
      "Epoch 8/40\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.0152 - accuracy: 0.9947\n",
      "Epoch 9/40\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.0169 - accuracy: 0.9942\n",
      "Epoch 10/40\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.0153 - accuracy: 0.9948\n",
      "Epoch 11/40\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.0153 - accuracy: 0.9948\n",
      "Epoch 12/40\n",
      "1500/1500 [==============================] - 14s 10ms/step - loss: 0.0145 - accuracy: 0.9949\n",
      "Epoch 13/40\n",
      "1500/1500 [==============================] - 15s 10ms/step - loss: 0.0134 - accuracy: 0.9956\n",
      "Epoch 14/40\n",
      "1500/1500 [==============================] - 15s 10ms/step - loss: 0.0145 - accuracy: 0.9949\n",
      "Epoch 15/40\n",
      "1500/1500 [==============================] - 14s 10ms/step - loss: 0.0126 - accuracy: 0.9957\n",
      "Epoch 16/40\n",
      "  58/1500 [>.............................] - ETA: 14s - loss: 0.0053 - accuracy: 0.9991"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_train_ravel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpos_feed_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kkart\\.conda\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kkart\\.conda\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n",
      "\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n",
      "\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n",
      "\u001b[0;32m   1562\u001b[0m ):\n",
      "\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n",
      "\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kkart\\.conda\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kkart\\.conda\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n",
      "\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kkart\\.conda\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n",
      "\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n",
      "\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n",
      "\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kkart\\.conda\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "\u001b[0;32m   2494\u001b[0m   (graph_function,\n",
      "\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n",
      "\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kkart\\.conda\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n",
      "\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n",
      "\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n",
      "\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n",
      "\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n",
      "\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n",
      "\u001b[0;32m   1865\u001b[0m     args,\n",
      "\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n",
      "\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n",
      "\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kkart\\.conda\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n",
      "\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n",
      "\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n",
      "\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n",
      "\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\kkart\\.conda\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n",
      "\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n",
      "\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loaded_model.fit([x_train_ravel,pos_feed_train],y_train,epochs=40,batch_size = 40,validation_split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 7ms/step - loss: 0.0438 - accuracy: 0.9885\n"
     ]
    }
   ],
   "source": [
    "out = loaded_model.evaluate([x_test_ravel,pos_feed_test],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the tensorflow model trans\n",
    "# loaded_model.save('ViT_validationAcc9867_updated.h5')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
